{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0hAW8ptqVeyP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rt1818/AI-ML-IIIT/blob/main/AIML_Probabilistic_ML_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Probabilistic ML models\n",
        "\n",
        "Topics:\n",
        "1. Convolutional Operation\n",
        "2. CNN and Using Learnt Representations\n",
        "3. CNN Visualization"
      ],
      "metadata": {
        "id": "V89R735GVNdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Convolutional Operations"
      ],
      "metadata": {
        "id": "0hAW8ptqVeyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Central to Convolutional Neural Networks (CNN), a convolution operation is a linear operation which involves element-wise multiplication between a small filter (say, a matrix of integers) and filter-sized patch from the image. We move this filter across the image like a sliding window from top left to bottom right. For each point on the image, a value is calculated based on the filter using a convolution operation. These filters can do simplest task like checking if there is a vertical line in the image or complicated task like detecting a human eye in the image.\n",
        "\n",
        "Let's look at the convolution formula:\n",
        "\n",
        "Convolution between image $f(x, y)$ and kernel $k(x, y)$ is\n",
        "$$f(x,y) * k(x,y) = \\sum \\limits _{i=0} ^{W-1} \\sum \\limits _{j=0} ^{H-1} f(i, j) k(x − i, y − j)$$\n",
        "\n",
        "where $W$ and $H$ are the the width and height of the image.\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Chaim-Baskin/publication/318849314/figure/fig1/AS:614287726870532@1523469015098/Image-convolution-with-an-input-image-of-size-7-7-and-a-filter-kernel-of-size-3-3.png\" alt=\"Convolution\" width=650px height=280px/>\n",
        "\n",
        "\n",
        "Image reference: [Streaming Architecture for Large-Scale Quantized Neural Networks on an FPGA-Based Dataflow Platform](https://www.researchgate.net/publication/318849314_Streaming_Architecture_for_Large-Scale_Quantized_Neural_Networks_on_an_FPGA-Based_Dataflow_Platform/figures?lo=1)\n",
        "\n",
        "The code demonstrates the convolution operation of a 2D matrix (image) with various filters"
      ],
      "metadata": {
        "id": "hbpRXyTpVv7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igR7HFGhRRdm"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D 3x3 binary image with vertical edge\n",
        "image1 = np.array([[1,1,0],\n",
        "                   [1,1,0],\n",
        "                   [1,1,0]])\n",
        "\n",
        "# 2D 3x3 binary image with horizontal edge\n",
        "image2 = np.array([[0,0,0],\n",
        "                   [0,0,0],\n",
        "                   [1,1,1]])\n",
        "\n",
        "# print(image1*255)\n",
        "# Let's plot the images\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "ax.imshow(image1, cmap='gray', extent=[0, 3, 3, 0])\n",
        "# plt.ylim(0, 3)\n",
        "ax.set_title('Image 1 with vertical edge')\n",
        "\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "ax.imshow(image2, cmap='gray', extent=[0, 3, 3, 0])\n",
        "ax.set_title('Image 2 with horizontal edge')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1WlakMr1Wlee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGZFq4O8-qql"
      },
      "source": [
        "Let's create a 3x3 vertical edge filter. We will 'convolve' this filter over the images to detect vertical edge. As the image is same size as of filter, this is simple element-wise multiplication and summing up the result into single value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ralw9w7R0x"
      },
      "source": [
        "# Vertical Line filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoWLcsu37jLq"
      },
      "source": [
        "# Applying filter to first image\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "# Applying filter to second image\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvCvYI9c-MX4"
      },
      "source": [
        "Non-zero output suggests that there is a vertical edge present in the first image and not present in the second image.\n",
        "Now, let's create a horizontal edge filter and apply it to both the above images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl7h6HrJ8h5W"
      },
      "source": [
        "# Horizontal edge filter\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BWlGBDVWEDO"
      },
      "source": [
        "As expected, the horizontal edge is detected in second image with this filter.\n",
        "\n",
        "Now, we will take a bigger image (5 x 5) and see how a convolution operation works by sliding a filter left to right and top to bottom to obtain an output map from image. Let's define a function ***apply_filter()*** for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VszqKiZHsOXB"
      },
      "source": [
        "def apply_filter(img, filter):\n",
        "  height, width = img.shape\n",
        "  filter_size = filter.shape\n",
        "\n",
        "  output = np.empty(0)\n",
        "\n",
        "  # Move the filter over entire image and store the result in output\n",
        "  for i in range(0, height - filter_size[1] + 1):\n",
        "    for j in range(0, width - filter_size[0] + 1):\n",
        "      # Matrix multiplication for a single patch of image and filter\n",
        "      output = np.append(output, np.sum(np.multiply(img[i:i+filter_size[0], j:j+filter_size[1]], filter)))\n",
        "\n",
        "  # Calculate the output shape of the resultant image\n",
        "  output_shape = (height - (filter_size[1]-1)), (width - (filter_size[0]-1))\n",
        "\n",
        "  # Return the reshaped image\n",
        "  return output.reshape(output_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y03Dtn3F0sp"
      },
      "source": [
        "Plotting function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mJqbXQZF0sq"
      },
      "source": [
        "def plot_images(images, titles, tick_params=True):\n",
        "  n = len(images)\n",
        "  fig = plt.figure(figsize=(10,4))\n",
        "  for i in range(n):\n",
        "    ax = fig.add_subplot(1,n,i+1)\n",
        "    if len(images[i].shape) == 2:\n",
        "      ax.imshow(images[i], cmap='gray',\n",
        "                extent=(0,images[i].shape[1], images[i].shape[0], 0))\n",
        "    else:\n",
        "      ax.imshow(images[i])\n",
        "    ax.set_title(titles[i])\n",
        "    if not tick_params:\n",
        "      plt.tick_params(axis='both', labelbottom=False, bottom=False,\n",
        "                labelleft=False, left=False)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpTLmPYG0Pom"
      },
      "source": [
        "# 2D image\n",
        "img = np.array([[20,20,0,0,0],\n",
        "                [20,20,0,0,0],\n",
        "                [20,20,0,0,0],\n",
        "                [20,20,0,0,0],\n",
        "                [20,20,0,0,0]])\n",
        "\n",
        "# Vertical edge filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "\n",
        "output = apply_filter(img, filter)\n",
        "print(output) # Note the shape of output image!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-tmlp1qHA9f"
      },
      "source": [
        "# Let's plot the above image with results\n",
        "images = []\n",
        "titles = []\n",
        "\n",
        "images.append(img)\n",
        "titles.append('Original Image')\n",
        "\n",
        "images.append(filter)\n",
        "titles.append('Filter')\n",
        "\n",
        "images.append(output)\n",
        "titles.append('Convolution Output')\n",
        "\n",
        "plot_images(images, titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSCGzaYVFze"
      },
      "source": [
        "As, you can see, horizontal edge is detected in the output.\n",
        "\n",
        "Now, we will see the effect of applying this filter on a grayscale image. Again, for this, we need to 'convolve' the filter over the entire image.\n",
        "We will use the same filter and function defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjOQM6tdF0sr"
      },
      "source": [
        "# Get the sample image\n",
        "!curl -L -o 'lotus.jpg' 'https://drive.google.com/uc?export=download&id=1gQSQlrUws22KLRUacXwvN1G8FtIyhfGt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04P_GfojIo4u"
      },
      "source": [
        "# Read the image with opencv, 0 stands for 'grayscale'\n",
        "image = cv2.imread('lotus.jpg', 0)\n",
        "print('Original image size: ', image.shape)\n",
        "\n",
        "# Saving images for plots\n",
        "images = []\n",
        "titles = []\n",
        "\n",
        "images.append(image)\n",
        "titles.append('Original Image')\n",
        "\n",
        "# Vertical edge filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "images.append(filter)\n",
        "titles.append('Filter')\n",
        "\n",
        "# Apply this filter to image\n",
        "output = apply_filter(image, filter)\n",
        "\n",
        "print('Output image size: ', output.shape)\n",
        "\n",
        "images.append(output)\n",
        "titles.append('Convolution Output')\n",
        "\n",
        "# Let's plot the images\n",
        "plot_images(images, titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "1. Try padding in convolution on lotus.jpg and show results\n",
        "2. Try stride  in convolution on lotus.jpg and show results"
      ],
      "metadata": {
        "id": "bsQxPFm-YDmI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. CNN and Using Learnt Representations\n",
        "\n",
        "Now lets implement a CNN in pytorch and use the learnt representations for image classification of MNIST dataset."
      ],
      "metadata": {
        "id": "u1hVe_w-aOKe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewhnwh1P6aAk"
      },
      "source": [
        "<img src='https://miro.medium.com/max/1872/1*SGPGG7oeSvVlV5sOSQ2iZw.png' />\n",
        "\n",
        "Image reference: https://miro.medium.com/max/1872/1*SGPGG7oeSvVlV5sOSQ2iZw.png\n",
        "\n",
        "We will be implementing a CNN model which can predict the digit, given a grayscale image. The architecture of model is given in the above image.\n",
        "\n",
        "**We will do the following steps in order:**\n",
        "1.   Load and visualize MNIST training and test datasets using torchvision\n",
        "2.   Define the CNN model\n",
        "3.   Define a loss function and optimizer\n",
        "4.   Train the network on the training data\n",
        "5.   Evaluate the network on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWJa9NQvSdr8"
      },
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhYuybK767Sa"
      },
      "source": [
        "# Device configuration (whether to run on GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZMFwB08ZvSN"
      },
      "source": [
        "# Set seeds for reproducibility\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ0n6wa47uZu"
      },
      "source": [
        "#### Load MNIST data\n",
        "We will use the [MNIST dataset](https://pytorch.org/vision/stable/datasets.html#mnist) from torchvision Pytorch and setup the train and test dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr2uSnxn7nIb"
      },
      "source": [
        "batch_size_train = 128\n",
        "batch_size_test = 128\n",
        "\n",
        "# Images in torchvision datasets are PIL Images in range [0,1] so we need\n",
        "# 'ToTensor' transform to convert them into tensors\n",
        "train_data = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
        "                             transform=torchvision.transforms.ToTensor())\n",
        "test_data = torchvision.datasets.MNIST('./data', train=False, download=True,\n",
        "                             transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HRj26o673We"
      },
      "source": [
        "#### Understand the dataset\n",
        "Let us now visualize the dataset in terms of number of samples, classes etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwP6Weyr7wZb"
      },
      "source": [
        "print('Training data shape : ', train_data.data.shape, train_data.targets.shape)\n",
        "print('Testing data shape : ', test_data.data.shape, test_data.targets.shape)\n",
        "\n",
        "# Find the unique numbers from the train labels\n",
        "classes = np.unique(train_data.targets.numpy())\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ZzTl-078NJ"
      },
      "source": [
        "# Helper function to plot data\n",
        "def plot_data(images, labels, classes=None):\n",
        "  figure = plt.figure(figsize=(9, 4))\n",
        "  cols, rows = 5, 2\n",
        "  for i in range(1, cols * rows + 1):\n",
        "      sample_idx = torch.randint(len(images), size=(1,)).item()\n",
        "      img, label = images[sample_idx], labels[sample_idx]\n",
        "      figure.add_subplot(rows, cols, i)\n",
        "      if classes is not None:\n",
        "        label = classes[label]\n",
        "      plt.title('Label:' +str(label))\n",
        "      plt.axis(\"off\")\n",
        "      plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1bnP8EyqktD"
      },
      "source": [
        "plot_data(train_data.data, train_data.targets.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYYNFnPk8LWD"
      },
      "source": [
        "#### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-e1DrVT8GYt"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # fully connected layer\n",
        "        self.fc = nn.Linear(64 * 7 * 7, 128)\n",
        "        # output layer 10 classes\n",
        "        self.out = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x) #activation\n",
        "        x = self.max_pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.max_pool2(x)\n",
        "        # flatten the output for FC layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        output = self.out(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqoGCFWd8RRG"
      },
      "source": [
        "# Build the model object and put on the device\n",
        "model = CNN().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux_2PGgT8diG"
      },
      "source": [
        "#### Define Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZDAwCNb8VER"
      },
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fwSTPmI8lQP"
      },
      "source": [
        "#### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GieENlOa8heQ"
      },
      "source": [
        "# Basic SGD optimizer with 0.01 learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svJ3-UB187qa"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t94F0wxMefKz"
      },
      "source": [
        "Helper function for training/testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDXNReyi8n5o"
      },
      "source": [
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Put data on devices\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFYI98OtNjCc"
      },
      "source": [
        "def test(model, test_loader):\n",
        "  # Eval mode\n",
        "  model.eval()\n",
        "  test_acc = 0\n",
        "  correct = 0\n",
        "  for i, (images, labels) in enumerate(test_loader):\n",
        "    # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "    with torch.no_grad():\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "      correct += (preds == labels).sum()\n",
        "\n",
        "  test_acc = correct / len(test_loader.dataset)\n",
        "  print('Test Accuracy: {:.4f}'.format(test_acc*100))\n",
        "\n",
        "  # Plot the images with predicted labels\n",
        "  plot_data(images.data.cpu().numpy(), preds.data.cpu().numpy(), test_loader.dataset.classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6pYjXgchc3E"
      },
      "source": [
        "Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax9XMoe5H_ik"
      },
      "source": [
        "num_epochs = 10  # iterations\n",
        "train_losses, train_acc = train(num_epochs, model, train_loader, loss_func, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6ltVbRI4dr6"
      },
      "source": [
        "Plot training plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPJHC1_tIFAr"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(train_losses)+1),train_losses)\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Loss vs Epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYW-_wTfWvYl"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rReuhwjpXr5K"
      },
      "source": [
        "# Evaluate the model on testing data and plot predictions\n",
        "test(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puMO_nSBmUFq"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfozvSgV_mt5"
      },
      "source": [
        "Q 1: What is the ratio of parameters in single 5 x 5 kernel and equivalent stacked 3 x 3 kernels? Consider number of channels in input and output channels as C."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7LXbTpZnbT0"
      },
      "source": [
        "Q 2: How can you replace 7 x 7 convolution kernel using only 3 x 3 kernels? What would be ratio of parameters in this case? Consider number of channels in input and output channels as C."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CNN Visualization"
      ],
      "metadata": {
        "id": "Tq_SyTxUbteW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga0Az1zLcPVz"
      },
      "source": [
        "Save the conv layers and their weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En_ZpAdpvVts"
      },
      "source": [
        "model_weights = [] # we will save the conv layer weights in this list\n",
        "conv_layers = [] # we will save the conv layers in this list\n",
        "# get all the model children as list\n",
        "model_children = list(model.children())\n",
        "\n",
        "# counter to keep count of the conv layers\n",
        "counter = 0\n",
        "# append all the conv layers and their respective weights to the list\n",
        "for i in range(len(model_children)):\n",
        "    if type(model_children[i]) == nn.Conv2d:\n",
        "        counter += 1\n",
        "        model_weights.append(model_children[i].weight)\n",
        "        conv_layers.append(model_children[i])\n",
        "    elif type(model_children[i]) == nn.Sequential:\n",
        "        for j in range(len(model_children[i])):\n",
        "            for child in model_children[i][j].children():\n",
        "                if type(child) == nn.Conv2d:\n",
        "                    counter += 1\n",
        "                    model_weights.append(child.weight)\n",
        "                    conv_layers.append(child)\n",
        "print(f\"Total convolutional layers: {counter}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N34IYC8Fziq1"
      },
      "source": [
        "# take a look at the conv layers and the respective weights\n",
        "for weight, conv in zip(model_weights, conv_layers):\n",
        "    # print(f\"WEIGHT: {weight} \\nSHAPE: {weight.shape}\")\n",
        "    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qhgNqhgcTpW"
      },
      "source": [
        "### Visualize the CONV layer filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCH5GBQpznZw"
      },
      "source": [
        "# Visualize the conv layer filters\n",
        "plt.figure(figsize=(20, 17))\n",
        "for i, filter in enumerate(model_weights[0]):\n",
        "    plt.subplot(8, 8, i+1) # (8, 8)\n",
        "    plt.imshow(filter[0, :, :].data.cpu().numpy(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UPNns-hcbWK"
      },
      "source": [
        "### Visualize filter outputs on an image\n",
        "Get an image from test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV_7rEx3zpuy"
      },
      "source": [
        "dataiter = iter(test_loader)\n",
        "for images, labels in dataiter:\n",
        "    img = images[1]\n",
        "    fig = plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(img.reshape((28, 28)))\n",
        "    print(classes[labels[1].item()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajfFCveAxpf-"
      },
      "source": [
        "Forward pass the image through saved conv layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR6LxW9t0ADw"
      },
      "source": [
        "results = [conv_layers[0](img.to(device))]\n",
        "for i in range(1, len(conv_layers)):\n",
        "    # pass the result from the last layer to the next layer\n",
        "    results.append(conv_layers[i](results[-1]))\n",
        "# make a copy of the `results`\n",
        "outputs = results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkeMeBQdxwpP"
      },
      "source": [
        "Visualize features from each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDuxp0nq0BEX"
      },
      "source": [
        "for num_layer in range(len(outputs)):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    layer_viz = outputs[num_layer][:, :, :]\n",
        "    layer_viz = layer_viz.data\n",
        "    print('Layer output size:', layer_viz.size())\n",
        "    for i, filter in enumerate(layer_viz):\n",
        "        plt.subplot(8, 8, i + 1)\n",
        "        plt.imshow(filter.cpu().numpy(), cmap='gray')\n",
        "        plt.axis(\"off\")\n",
        "    print(f\"Layer {num_layer} feature maps...\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oniT6gixQZ7_"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5XatV34Qia0"
      },
      "source": [
        "Q: List a few practical applications of convolutional autoencoders."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional autoencoders (CAEs) are a type of neural network architecture that combines convolutional layers with autoencoder structures. They have various practical applications across different domains. Here are a few examples:\n",
        "\n",
        "Image Compression:\n",
        "CAEs can be used for compressing images while preserving important features. By training the network to reconstruct high-quality images from compressed representations, CAEs enable efficient image compression for storage and transmission purposes.\n",
        "\n",
        "Image Denoising:\n",
        "CAEs can be trained to remove noise from images. By learning to encode the clean structure of an image and then decode it, the network can effectively denoise images, which is useful in medical imaging, surveillance, or any application where noisy images are a concern.\n",
        "\n",
        "Feature Learning:\n",
        "CAEs are effective at learning hierarchical representations of features in images. These learned features can be used for tasks such as image recognition, object detection, and segmentation. The encoder part of the CAE can be used as a feature extractor."
      ],
      "metadata": {
        "id": "13dmH25VJqhY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqMzkKLBq18x"
      },
      "source": [
        "Q: What change do we need to make for the autoencoder to reduce into PCA?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Linear Activation and Loss Function:\n",
        "# Use linear activation functions for the output layer of both the encoder and decoder. This ensures that the network can learn to reconstruct the input data in a linear manner.\n",
        "# Employ a mean squared error (MSE) loss function, which is common for regression problems and aligns with the goal of minimizing the reconstruction error.\n",
        "\n",
        "# Example in Keras/TensorFlow\n",
        "\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "# Set the size of your input data\n",
        "input_size = 784  # Replace with the actual size of your input data\n",
        "\n",
        "# Set the desired dimensionality for encoding\n",
        "desired_dimensionality = 32  # Replace with the desired dimensionality\n",
        "\n",
        "input_data = Input(shape=(input_size,))\n",
        "encoded = Dense(desired_dimensionality, activation='linear')(input_data)\n",
        "decoded = Dense(input_size, activation='linear')(encoded)\n",
        "\n",
        "autoencoder = Model(input_data, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n"
      ],
      "metadata": {
        "id": "ZbMauVBaKLNd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0Rm8FB-Qn4K"
      },
      "source": [
        "## References and Additional Resources:\n",
        "\n",
        "*   [Training a classifier tutorial - Pytorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-an-image-classifier)\n",
        "*   [Visualizing Filters and Feature Maps in Convolutional Neural Networks using PyTorch](https://debuggercafe.com/visualizing-filters-and-feature-maps-in-convolutional-neural-networks-using-pytorch/)\n",
        "*   [ConvNetJS CIFAR10 Demo](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)\n",
        "\n",
        "\n"
      ]
    }
  ]
}